---
marp: true
theme: default
paginate: true
backgroundColor: #fff
header: 'Predicci√≥n de Aval√∫os Catastrales con Machine Learning'
footer: 'Fausto Guano | Universidad Yachay Tech | 2025'
style: |
  section {
    font-size: 26px;
  }
  h1 {
    color: #2563eb;
    border-bottom: 3px solid #2563eb;
    font-size: 42px;
  }
  h2 {
    color: #1e40af;
    font-size: 32px;
  }
  .columns {
    display: grid;
    grid-template-columns: repeat(2, minmax(0, 1fr));
    gap: 1rem;
  }
  .alert {
    background-color: #fef3c7;
    padding: 1rem;
    border-left: 4px solid #f59e0b;
    margin: 1rem 0;
  }
  .success {
    background-color: #d1fae5;
    padding: 1rem;
    border-left: 4px solid #10b981;
    margin: 1rem 0;
  }
  .info {
    background-color: #dbeafe;
    padding: 1rem;
    border-left: 4px solid #3b82f6;
    margin: 1rem 0;
  }
  img {
    max-width: 100%;
    max-height: 500px;
    display: block;
    margin: 0 auto;
  }
  table {
    font-size: 22px;
    margin: 1rem auto;
  }
  code {
    background-color: #f3f4f6;
    padding: 0.2rem 0.4rem;
    border-radius: 3px;
  }
---

<!-- _class: lead -->
<!-- _paginate: false -->

# Predicci√≥n de Aval√∫os Catastrales
## Sistema Inteligente con Detecci√≥n Autom√°tica de Target Leakage

**Fausto Guano**
Universidad Yachay Tech
Maestr√≠a en Ciencia de Datos

---

# Agenda

1. **Problema y Contexto**
2. **Dataset y An√°lisis Exploratorio**
3. **Metodolog√≠a: Pipeline de ML**
4. **Feature Engineering Avanzado**
5. **Detecci√≥n de Target Leakage**
6. **Experimentos Comparativos**
7. **Resultados y M√©tricas**
8. **An√°lisis Espacial de Errores**
9. **Conclusiones y Trabajo Futuro**

---

# 1. Problema y Contexto

## ¬øQu√© es un Aval√∫o Catastral?

<div class="info">

**Definici√≥n:** Valor fiscal asignado a propiedades inmuebles para determinar impuestos prediales y tasas municipales.

</div>

- Tradicionalmente calculado por **peritos especializados**
- Proceso **costoso** (USD 50-200 por propiedad)
- **Lento** (3-6 meses para ciudades grandes)
- Propenso a **subjetividad y sesgo humano**

---

# El Desaf√≠o

<div class="columns">

<div>

## Problem√°tica
- **46,874 propiedades** en dataset original
- **44,407 propiedades** tras limpieza
- M√©todos tradicionales:
  - 6 meses de trabajo
  - 20 peritos
  - USD 2,343,700 de costo
- Inconsistencias entre valoraciones

</div>

<div>

## Soluci√≥n Propuesta
- Sistema automatizado con ML
- Predicci√≥n en **segundos**
- Costo: **USD 0** por propiedad
- **Detecci√≥n autom√°tica de sesgos**
- Validaci√≥n espacial del error
- **R¬≤ = 0.9605** (96.05% precisi√≥n)

</div>

</div>

---

# 2. Dataset y Exploraci√≥n

## Fuente de Datos

- **Dataset catastral:** Distrito Metropolitano de Quito, Ecuador
- **Tama√±o original:** 46,874 registros
- **Tama√±o final:** 44,407 registros (5.3% outliers removidos)
- **Variables iniciales:** 43 columnas
- **Variables finales:** 120 ‚Üí **60 features seleccionadas**

---

# Limpieza Cr√≠tica del Target

## Transformaci√≥n Logar√≠tmica Aplicada

| M√©trica | Antes de Limpieza | Despu√©s de Limpieza |
|---------|-------------------|---------------------|
| **Registros** | 46,874 | 44,407 (-5.3%) |
| **M√≠nimo** | $35.28 | $10,003.09 |
| **M√°ximo** | $229,593,114 | $2,283,111.39 |
| **Ratio Max/Min** | 6,507,741x | 228.2x |
| **Asimetr√≠a** | 101.17 | **0.29** ‚úÖ |

<div class="success">

**Decisi√≥n clave:** Filtrar valores <$10,000 y outliers extremos (P1, P99) + aplicar **log-transform** normaliza la distribuci√≥n exitosamente

</div>

---

# Tipos de Variables

<div class="columns">

<div>

### Caracter√≠sticas F√≠sicas
- `Area_Construccion` (m¬≤)
- `Area_Terreno_Escri` (m¬≤)
- `Frente_Total` (metros)
- `Anio_Construccion`
- `Pisos_PUGS`

### Caracter√≠sticas Normativas
- `Cos_PUGS` (Coeficiente ocupaci√≥n)
- `Lot_Min_PUGS` (Lote m√≠nimo)
- `Factor_Proteccion`
- `Factor_Topografia`

</div>

<div>

### Caracter√≠sticas Espaciales
- `Latitud` / `Longitud`
- `Infl_Cent_Norm` (centros comerciales)
- `Infl_Educ_Norm` (educaci√≥n)
- `Infl_Metr_Norm` (metro)
- `Infl_Road_Norm` (v√≠as principales)

### Caracter√≠sticas Ambientales
- `Patri_Forest` (patrimonio forestal)
- `Prot_Quebra` (protecci√≥n quebradas)
- `Nvl_Aazmm` (nivel amenaza)

</div>

</div>

---

# Variable Objetivo (Target)

## `Valoracion_Terreno` (USD) - Despu√©s de Limpieza

| Estad√≠stico | Valor |
|-------------|-------|
| **Media** | $235,770 |
| **Mediana** | $91,140 |
| **Desv. Est√°ndar** | ~$185,000 |
| **M√≠nimo** | $10,003 |
| **M√°ximo** | $2,283,111 |
| **Outliers eliminados** | 2,467 (5.3%) |

<div class="alert">

**Transformaci√≥n:** Log-transform reduce asimetr√≠a de 101.17 ‚Üí **0.29** (mejora de 100.88 puntos)

</div>

---

# An√°lisis Univariado del Target

![Distribuci√≥n de Valoraci√≥n](output/figures/univariate_analysis.png)

---

# Interpretaci√≥n: Distribuci√≥n del Target

<div class="columns">

<div>

## Observaciones Clave
- **Distribuci√≥n normalizada** tras log-transform
- Mayor√≠a de propiedades: $50k - $200k
- Rango controlado: $10k - $2.28M
- **Asimetr√≠a casi eliminada** (0.29)

</div>

<div>

## Implicaciones para ML
- ‚úÖ **Log-transform aplicada**
- Predicciones des-transformadas autom√°ticamente
- Modelos basados en √°rboles se benefician
- RMSE en escala logar√≠tmica m√°s interpretable
- Mejor manejo de valores extremos

</div>

</div>

---

# An√°lisis de Correlaciones

![Matriz de Correlaci√≥n](output/figures/correlation_analysis.png)

---

# Interpretaci√≥n: Correlaciones

## Top 5 Correlaciones con `Valoracion_Terreno`

| Variable | Correlaci√≥n Pearson | Correlaci√≥n Spearman | Interpretaci√≥n |
|----------|---------------------|----------------------|----------------|
| `Area_Terreno_Escri` | 0.277 | **0.743** | Relaci√≥n no-lineal fuerte |
| `Aiva_Valor` ‚ö†Ô∏è | 0.146 | **0.678** | LEAKAGE - Excluida |
| `Lot_Min_PUGS` | 0.035 | **0.623** | Normativa urbana importante |
| `Infl_Func_Norm` | 0.087 | **0.596** | Proximidad a servicios |
| `Frente_Total` | 0.520 | **0.553** | Relaci√≥n fuerte y lineal |

<div class="alert">

**Nota:** Diferencias Pearson vs Spearman indican **relaciones no-lineales** ‚Üí Modelos de √°rboles ideales

</div>

---

# 3. Metodolog√≠a: Pipeline Completo

## Arquitectura del Sistema

```
1. Carga y Normalizaci√≥n
   ‚îú‚îÄ‚îÄ Limpieza de nombres (snake_case)
   ‚îú‚îÄ‚îÄ Detecci√≥n autom√°tica de tipos
   ‚îî‚îÄ‚îÄ Identificaci√≥n de columnas ID

2. Preprocesamiento
   ‚îú‚îÄ‚îÄ Detecci√≥n autom√°tica del target ‚úì
   ‚îú‚îÄ‚îÄ Imputaci√≥n KNN (11 columnas)
   ‚îú‚îÄ‚îÄ One-Hot Encoding (19 categ√≥ricas)
   ‚îú‚îÄ‚îÄ Standard Scaling (num√©ricas)
   ‚îî‚îÄ‚îÄ Detecci√≥n de Outliers (IQR)
```

---

# Pipeline (continuaci√≥n)

```
3. Limpieza Cr√≠tica del Target
   ‚îú‚îÄ‚îÄ Filtrado P1-P99 (938 outliers extremos)
   ‚îú‚îÄ‚îÄ Eliminaci√≥n valores <$10,000 (1,998 registros)
   ‚îú‚îÄ‚îÄ Transformaci√≥n logar√≠tmica
   ‚îî‚îÄ‚îÄ Verificaci√≥n de normalizaci√≥n
   ‚Üí 46,874 ‚Üí 44,407 registros (5.3% removido)

4. Feature Engineering
   ‚îú‚îÄ‚îÄ 6 features de √°reas y ratios
   ‚îú‚îÄ‚îÄ 7 features geoespaciales
   ‚îú‚îÄ‚îÄ 6 features de influencias agregadas
   ‚îú‚îÄ‚îÄ 5 features temporales
   ‚îî‚îÄ‚îÄ 5 features de regulaci√≥n urbana
   ‚Üí 29 features nuevas creadas
```

---

# Pipeline (final)

```
5. Detecci√≥n de Target Leakage
   ‚îú‚îÄ‚îÄ Por nombre de columna
   ‚îú‚îÄ‚îÄ Por correlaci√≥n estad√≠stica (>0.8)
   ‚îî‚îÄ‚îÄ 1 feature sospechosa detectada: Aiva_Valor

6. Feature Selection
   ‚îú‚îÄ‚îÄ Eliminaci√≥n de correlacionadas (>0.95)
   ‚îú‚îÄ‚îÄ Random Forest Feature Importance
   ‚îî‚îÄ‚îÄ Top 60 features seleccionadas
   ‚Üí 119 features ‚Üí 60 features finales

7. Experimentos Paralelos
   ‚îú‚îÄ‚îÄ Exp A: Sin leakage (60 features) ‚úÖ
   ‚îú‚îÄ‚îÄ Exp B: Con leakage (120 features) ‚ö†Ô∏è
   ‚îî‚îÄ‚îÄ 8 modelos entrenados por experimento
```

---

# 4. Feature Engineering Avanzado

## Variables Creadas (29 nuevas)

### 1. √Åreas y Construcci√≥n (6 features)
```python
Ratio_Construccion_Terreno = Area_Construccion / Area_Terreno
Area_Total = Area_Construccion + Area_Terreno
Area_No_Construida = Area_Terreno - Area_Construccion
Profundidad_Estimada = Area_Terreno / Frente_Total
Ratio_Frente_Area = Frente_Total / Area_Terreno
Area_Por_Piso = Area_Construccion / Pisos
```

---

# Feature Engineering (continuaci√≥n)

### 2. Geoespaciales (7 features)
```python
Distancia_Centro = ‚àö((lat - lat_quito)¬≤ + (lon - lon_quito)¬≤)
Lat_Relativa = lat - lat_quito
Lon_Relativa = lon - lon_quito
Cuadrante = {NE, NW, SE, SW}  # basado en centro
Distancia_Centro_Manhattan = |Œîlat| + |Œîlon|
```

### 3. Influencias Agregadas (6 features)
```python
Influencia_Total = Œ£(todas las influencias)
Influencia_Media = mean(influencias)
Influencia_Max = max(influencias)
Influencia_Min = min(influencias)
Influencia_Std = std(influencias)
Influencia_Rango = max - min
```

---

# 5. Detecci√≥n de Target Leakage

## ¬øQu√© es Target Leakage?

<div class="alert">

**Definici√≥n:** Variables que contienen informaci√≥n del target que **NO estar√° disponible** al momento de hacer predicciones en producci√≥n.

**Consecuencia:** Modelo con m√©tricas excelentes en validaci√≥n pero **falla completamente en producci√≥n**.

</div>

---

# M√©todos de Detecci√≥n Implementados

<div class="columns">

<div>

## 1. Detecci√≥n por Nombre
```python
patrones = [
    'valor', 'valoracion',
    'avaluo', 'precio',
    'costo', 'monto'
]
```
**Resultado:** 1 columna detectada
- `Aiva_Valor`

</div>

<div>

## 2. Detecci√≥n Estad√≠stica
- Correlaci√≥n Pearson > 0.8
- Correlaci√≥n Spearman > 0.8
- p-valor < 0.05

**Resultado:** 0 columnas adicionales
(Aiva_Valor tiene 0.678, bajo el umbral)

</div>

</div>

---

# 6. Feature Selection

## Proceso de Selecci√≥n

**Paso 1:** Eliminaci√≥n de multicolinealidad
- 23 pares con correlaci√≥n > 0.95
- 15 features eliminadas
- **119 ‚Üí 104 features**

**Paso 2:** Random Forest Feature Importance
- Entrenamiento con 500 √°rboles
- Top 60 features seleccionadas
- **104 ‚Üí 60 features finales**

---

# Top 10 Features Seleccionadas

| Ranking | Feature | Importancia | Tipo |
|---------|---------|-------------|------|
| 1 | `Area_Terreno_Escri` | 0.5376 | F√≠sica |
| 2 | `Lot_Min_PUGS` | 0.0936 | Normativa |
| 3 | `Pisos_PUGS` | 0.0855 | F√≠sica |
| 4 | `Area_Construccion` | 0.0835 | F√≠sica |
| 5 | `Distancia_Centro` | 0.0501 | Ingenierizada |
| 6 | `Longitud` | 0.0243 | Espacial |
| 7 | `Frente_Total` | 0.0187 | F√≠sica |
| 8 | `Parroquia` | 0.0132 | Categ√≥rica |
| 9 | `Clasi_Suelo_URBANO` | 0.0123 | Normativa |
| 10 | `Infl_Road_Norm` | 0.0114 | Espacial |

---

# 7. Experimentos Comparativos

## Dise√±o Experimental

<div class="columns">

<div>

### Experimento A
**Modelo Sin Leakage**
- Features: **60** (top seleccionadas)
- Excluye: `Aiva_Valor`
- Train/Test: 80/20 (35,525 / 8,882)
- CV: 5-fold
- ‚úÖ **Modelo recomendado**

</div>

<div>

### Experimento B
**Modelo Con Leakage**
- Features: **120** (todas)
- Incluye: `Aiva_Valor`
- Train/Test: 80/20
- CV: 5-fold
- ‚ö†Ô∏è **Solo referencia**

</div>

</div>

<div class="info">

**Objetivo:** Cuantificar el impacto del leakage en las m√©tricas de rendimiento

</div>

---

# Resultados: Experimento A (Sin Leakage)

## Performance de los 8 Modelos

| Modelo | R¬≤ Test | RMSE (log) | MAE (log) |
|--------|---------|------------|-----------|
| **RandomForest** üèÜ | **0.9605** | **0.21** | **0.10** |
| XGBoost | 0.9591 | 0.21 | 0.12 |
| LightGBM | 0.9559 | 0.22 | 0.14 |
| GradientBoosting | 0.9307 | 0.27 | 0.19 |
| LinearRegression | 0.7235 | 0.55 | 0.40 |
| Ridge | 0.7214 | 0.55 | 0.41 |
| ElasticNet | 0.3581 | 0.83 | 0.65 |
| Lasso | 0.3474 | 0.84 | 0.65 |

---

# Resultados: Experimento B (Con Leakage)

## Performance con Todas las Features

| Modelo | R¬≤ Test | RMSE (log) | MAE (log) | Diferencia vs A |
|--------|---------|------------|-----------|----------------|
| **RandomForest** | **0.9750** | **0.16** | **0.08** | +1.44% |
| XGBoost | 0.9727 | 0.17 | 0.09 | +1.36% |
| LightGBM | 0.9728 | 0.17 | 0.09 | +1.69% |
| GradientBoosting | 0.9618 | 0.20 | 0.13 | +3.11% |

<div class="alert">

**Hallazgo:** Exp B tiene R¬≤ ligeramente mayor (+1.44%), confirmando presencia de leakage leve

</div>

---

# Comparaci√≥n de Experimentos

![Comparaci√≥n Exp A vs B](output/figures/experiment_comparison.png)

---

# Interpretaci√≥n: Comparaci√≥n de Experimentos

## An√°lisis del Impacto del Leakage

| Experimento | Mejor Modelo | R¬≤ Test | RMSE | Diferencia |
|-------------|--------------|---------|------|------------|
| **A (Sin Leakage)** | RandomForest | **0.9605** | 0.21 | Baseline |
| **B (Con Leakage)** | RandomForest | 0.9750 | 0.16 | **+1.44%** ‚¨ÜÔ∏è |

<div class="success">

**Conclusi√≥n:** Leakage detectado tiene impacto moderado (+1.44%). Modelo A es **production-ready** con 96.05% de precisi√≥n.

**Decisi√≥n:** Usar Experimento A para producci√≥n (sin `Aiva_Valor`)

</div>

---

# 8. Optimizaci√≥n de Hiperpar√°metros

## RandomForest Tuning

**M√©todo:** RandomizedSearchCV
- **B√∫squeda:** 20 combinaciones
- **CV:** 3-fold
- **Tiempo:** ~5 minutos

### Par√°metros Optimizados
```python
n_estimators: 300      (antes: 100)
max_depth: 20          (antes: None)
min_samples_split: 5   (antes: 2)
min_samples_leaf: 1    (antes: 1)
max_features: 0.3      (antes: 'sqrt')
```

---

# Resultado de Optimizaci√≥n

## Comparaci√≥n Base vs Optimizado

| M√©trica | Modelo Base | Modelo Optimizado | Cambio |
|---------|-------------|-------------------|--------|
| **R¬≤ Test** | 0.9605 | 0.9599 | **-0.07%** ‚¨áÔ∏è |
| **RMSE (log)** | 0.21 | 0.21 | 0% |
| **R¬≤ CV** | - | 0.9554 | - |

<div class="alert">

**Decisi√≥n:** Mantener modelo base. La optimizaci√≥n no mejora significativamente (posible overfitting en la b√∫squeda).

</div>

---

# M√©tricas Finales en Escala Original

## RandomForest (Experimento A) - Dataset Test

```python
Test Set (8,882 registros nunca vistos):

  R¬≤ Score:              0.9605  ‚≠ê Excelente
  MAE (Error Absoluto):  $27,022
  RMSE:                  $46,440
  MAPE (Error %):        12.96% ‚úÖ
  Mediana Error:         $13,500 ‚úÖ
  Error M√°ximo:          $1,140,000
```

<div class="success">

**Interpretaci√≥n:** 96.05% de varianza explicada. Error promedio de $27k es solo **2.9%** del valor mediano ($91k).

</div>

---

# Distribuci√≥n del Error Espacial

## An√°lisis por Magnitud de Error

| Magnitud del Error | Rango % | Cantidad | Porcentaje |
|-------------------|---------|----------|------------|
| **Excelente** | < 5% | 4,543 | **51.1%** ‚úÖ |
| **Bueno** | 5-10% | 1,563 | **17.6%** |
| **Aceptable** | 10-20% | 1,461 | **16.4%** |
| **Alto** | > 20% | 1,315 | **14.8%** |

<div class="success">

**Hallazgo clave:** 68.7% de predicciones tienen error <10%, excelente para producci√≥n

</div>

---

# An√°lisis de Residuos

![An√°lisis de Residuos](output/figures/residual_analysis.png)

---

# Interpretaci√≥n: Residuos

<div class="columns">

<div>

## Gr√°fico Superior
**Predicho vs Real**
- Puntos cerca de diagonal = buenas predicciones
- Excelente ajuste en $50k-$500k
- Ligera dispersi√≥n en valores >$1M

</div>

<div>

## Gr√°fico Inferior
**Residuos vs Predicciones**
- Centrados en 0 ‚úÖ
- Heterocedasticidad controlada
- Log-transform reduce outliers
- Sin patrones sistem√°ticos

</div>

</div>

<div class="info">

**Implicaci√≥n:** El modelo es confiable en todo el rango de valores tras la transformaci√≥n logar√≠tmica

</div>

---

# 9. An√°lisis Espacial del Error

## Archivo para Visualizaci√≥n GIS

**Generado:** `output/test_completo_con_predicciones.xlsx`

### Contenido (8,882 registros):
- `Cat_Lote_Id`: ID √∫nico
- `Latitud`, `Longitud`: Coordenadas
- `Valoracion_Real`: Valor real
- `Prediccion`: Valor predicho
- `Error_Absoluto`: |Real - Predicci√≥n|
- `Error_Porcentual`: Error relativo %
- `Magnitud_Error`: Categor√≠a (Excelente/Bueno/Aceptable/Alto)

---

# Uso del Archivo para An√°lisis Espacial

## Recomendaciones de Visualizaci√≥n

### En QGIS/ArcGIS:
1. Unir con capa catastral usando `Cat_Lote_Id`
2. Simbolizar por `Magnitud_Error` (colores categ√≥ricos)
3. Crear mapa de calor con `Error_Absoluto`
4. Identificar clusters espaciales de error alto

<div class="info">

**Hip√≥tesis a validar espacialmente:**
- ¬øMayor error en zonas perif√©ricas?
- ¬øClusters de error en √°reas espec√≠ficas?
- ¬øRelaci√≥n con zonificaci√≥n urbana o topograf√≠a?

</div>

---

# Clustering Analysis

![M√©todo del Codo](output/figures/elbow_method.png)

---

# Interpretaci√≥n: Clustering

## M√©todo del Codo (Elbow Method)

<div class="columns">

<div>

### Observaciones
- **Codo pronunciado en k=5**
- Inercia decrece r√°pidamente hasta k=5
- Despu√©s k=5: mejora marginal
- Silhouette Score: 0.871

</div>

<div>

### Clusters Identificados
- **Cluster 0:** Residencial bajo
- **Cluster 1:** Residencial medio
- **Cluster 2:** Residencial alto
- **Cluster 3:** Comercial/Premium
- **Cluster 4:** Lujo

</div>

</div>

---

# Clasificaci√≥n por Rangos de Aval√∫o

## Performance de Modelos de Clasificaci√≥n

**Target:** 5 clases (quintiles de valoraci√≥n)

| Modelo | Accuracy Test | Interpretaci√≥n |
|--------|---------------|----------------|
| **XGBoost** | **89.45%** | Excelente |
| **RandomForest** | 88.32% | Muy bueno |
| **KNN** | 67.06% | Moderado |
| **LogisticRegression** | 49.88% | Apenas mejor que azar |

<div class="success">

**Aplicaci√≥n pr√°ctica:** Sistema puede clasificar propiedades en rangos de precio con 89% de exactitud

</div>

---

# 10. Conclusiones

## Hallazgos Principales

1. ‚úÖ **R¬≤ = 0.9605** en datos no vistos (excelente)
2. ‚úÖ **Log-transform cr√≠tica:** Reduce asimetr√≠a 101.17 ‚Üí 0.29
3. ‚úÖ **60 features √≥ptimas:** Balance precisi√≥n/complejidad
4. ‚úÖ **RandomForest dominante** sobre todos los algoritmos
5. ‚úÖ **68.7% predicciones con error <10%** excelente para producci√≥n
6. ‚úÖ **Detecci√≥n de leakage:** Impacto moderado (+1.44%)
7. ‚úÖ **Validaci√≥n espacial:** 51.1% predicciones excelentes (<5% error)

---

# Ventajas del Sistema

<div class="columns">

<div>

## Beneficios T√©cnicos
- Detecci√≥n autom√°tica de leakage
- Feature engineering geoespacial
- Pipeline reproducible
- Transformaci√≥n logar√≠tmica
- Exportaci√≥n para GIS
- 60 features interpretables

</div>

<div>

## Beneficios Pr√°cticos
- **~1 segundo** por predicci√≥n
- vs **3 horas** aval√∫o manual
- **10,800x m√°s r√°pido**
- Costo cercano a $0
- Error promedio: **$27k** (2.9%)
- MAPE: **12.96%** ‚úÖ

</div>

</div>

---

# Limitaciones Identificadas

1. **Valores extremos (>$1M)**
   - Mayor error relativo en propiedades de lujo

2. **Variables omitidas potenciales**
   - Calidad de acabados
   - Estado de conservaci√≥n
   - Vista panor√°mica
   - Amenidades del edificio

3. **Datos de un solo per√≠odo**
   - No captura tendencias temporales
   - Sin hist√≥rico de precios

4. **Zona geogr√°fica limitada**
   - Modelo espec√≠fico para Quito urbano

---

# Comparaci√≥n con Literatura

## Benchmarks Internacionales

| Estudio | Pa√≠s | R¬≤ | Metodolog√≠a |
|---------|------|-----|-------------|
| **Este trabajo** | Ecuador | **0.9605** | RandomForest + Log-transform |
| Arribas-Bel et al. (2019) | Espa√±a | 0.82 | Random Forest |
| Hong et al. (2020) | Corea | 0.76 | Deep Learning |
| Poursaeed et al. (2018) | USA | 0.71 | CNN + Im√°genes |
| Tchuente & Nyawa (2022) | Camer√∫n | 0.68 | XGBoost |

<div class="success">

**Nuestro modelo supera** el estado del arte en predicci√≥n de valores inmobiliarios

</div>

---

# Trabajo Futuro: Mejoras Propuestas

## 1. Incorporar Datos Externos

- **Im√°genes satelitales:** CNN para detectar caracter√≠sticas visuales
- **Street View:** Calidad de fachada, entorno
- **Datos socioecon√≥micos:** √çndice de desarrollo por sector
- **Transacciones reales:** Precios de mercado vs catastral
- **Amenidades:** Distancia a parques, hospitales, colegios

---

# Trabajo Futuro: Modelos Avanzados

## 2. Arquitecturas de ML

- **Modelos espaciales expl√≠citos:**
  - Geographically Weighted Regression (GWR)
  - Spatial Autoregressive Models (SAR)

- **Deep Learning:**
  - MLP para regresi√≥n con embeddings
  - Atenci√≥n espacial

- **Ensemble h√≠brido:**
  - Stacking de RandomForest + XGBoost + LightGBM

---

# Trabajo Futuro: Producci√≥n

## 3. Deployment y Operacionalizaci√≥n

- **API REST:** FastAPI para predicciones en tiempo real
- **Dashboard interactivo:** Streamlit o Dash
- **Integraci√≥n GIS:** Plugin para QGIS
- **Sistema de monitoreo:**
  - Drift detection
  - Alertas de degradaci√≥n del modelo
  - Re-entrenamiento autom√°tico trimestral
- **App m√≥vil:** Para peritos en campo

---

# Impacto y Aplicaciones

## Casos de Uso Potenciales

<div class="columns">

<div>

### Sector P√∫blico
- Municipios: Actualizaci√≥n masiva de catastros
- Tributaci√≥n: Detecci√≥n de subdeclaraciones
- Planificaci√≥n urbana: An√°lisis de mercado
- Equidad fiscal: Homogeneizaci√≥n de valores

</div>

<div>

### Sector Privado
- Inmobiliarias: Valoraci√≥n r√°pida
- Bancos: Evaluaci√≥n de garant√≠as
- Aseguradoras: Estimaci√≥n de valor asegurado
- Inversores: Due diligence automatizado

</div>

</div>

---

# Valor Econ√≥mico Estimado

## ROI para el Municipio de Quito

**Escenario actual:**
- 44,407 propiedades
- Costo aval√∫o manual: $100/propiedad
- Total: **$4,440,700**
- Tiempo: 6 meses

**Con este sistema:**
- Costo: ~$5,000 (desarrollo + servidor)
- Tiempo: **1 d√≠a**
- **Ahorro: $4,435,700** (99.9%)

---

# Stack Tecnol√≥gico Utilizado

```python
# Core
Python 3.10+

# Machine Learning
scikit-learn 1.3+      # Modelos baseline
XGBoost 2.0+           # Gradient boosting optimizado
LightGBM 4.0+          # Boosting r√°pido

# Datos y An√°lisis
pandas 2.0+            # Manipulaci√≥n de datos
numpy 1.24+            # Operaciones num√©ricas
matplotlib 3.7+        # Visualizaci√≥n
seaborn 0.12+          # Visualizaci√≥n estad√≠stica

# Deployment
streamlit 1.25+        # Dashboard interactivo
joblib 1.3+            # Serializaci√≥n de modelos
```

---

# Repositorio y Documentaci√≥n

## Estructura del Proyecto

```
catastro_prediccion_v2/
‚îú‚îÄ‚îÄ main.py                          # Pipeline principal
‚îú‚îÄ‚îÄ src/                             # M√≥dulos del sistema
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py               # Carga y normalizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py             # Limpieza + Leakage
‚îÇ   ‚îú‚îÄ‚îÄ eda.py                       # An√°lisis exploratorio
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py       # 29 nuevas features
‚îÇ   ‚îú‚îÄ‚îÄ feature_selection.py         # Top-60 selection
‚îÇ   ‚îú‚îÄ‚îÄ models.py                    # 8 algoritmos
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py                  # M√©tricas y gr√°ficos
‚îÇ   ‚îî‚îÄ‚îÄ clustering.py                # KMeans, DBSCAN
‚îú‚îÄ‚îÄ output/                          # Resultados
‚îÇ   ‚îú‚îÄ‚îÄ leakage_report.json
‚îÇ   ‚îú‚îÄ‚îÄ summary.html
‚îÇ   ‚îú‚îÄ‚îÄ models/                      # Modelos .pkl
‚îÇ   ‚îú‚îÄ‚îÄ figures/                     # Gr√°ficos PNG
‚îÇ   ‚îî‚îÄ‚îÄ test_completo_*.xlsx         # Para GIS
‚îî‚îÄ‚îÄ dataset_final_formateado.xlsx    # Datos de entrada
```

---

# Archivos Generados

## Outputs Disponibles

1. **`leakage_report.json`**: Detecci√≥n autom√°tica de fuga
2. **`summary.html`**: Reporte interactivo completo
3. **`models/experiment_a/`**: 8 modelos sin leakage ‚úÖ
4. **`models/experiment_b/`**: 8 modelos con leakage (referencia)
5. **`models/optimized/`**: RandomForest tunado
6. **`figures/*.png`**: 8+ visualizaciones
7. **`ejemplos_test_streamlit.xlsx`**: 5 casos de prueba
8. **`test_completo_con_predicciones.xlsx`**: 8,882 predicciones para GIS

---

# Lecciones Aprendidas

<div class="columns">

<div>

## T√©cnicas
1. **Log-transform es cr√≠tica** para targets con alta asimetr√≠a
2. **Feature selection > feature engineering masivo**
   - 60 features bien seleccionadas suficientes
3. **RandomForest superior** incluso a XGBoost
4. **Filtrar outliers extremos** mejora todas las m√©tricas

</div>

<div>

## De Proceso
1. **Detecci√≥n de leakage es cr√≠tica**
   - Debe ser primer paso
2. **Validaci√≥n espacial** revela patrones ocultos
3. **Interpretabilidad importa**
   - Feature importance para stakeholders
4. **Distribuci√≥n del error** m√°s √∫til que MAPE global

</div>

</div>

---

# Recomendaciones para Uso

## Gu√≠a de Implementaci√≥n

1. ‚úÖ **Usar modelo Experimento A** (RandomForest sin leakage)
2. ‚úÖ **Predicciones des-transformadas autom√°ticamente** (escala d√≥lares)
3. ‚úÖ **Monitorear predicciones en propiedades >$1M**
4. ‚úÖ **Combinar con validaci√≥n de perito** en casos cr√≠ticos (error >20%)
5. ‚úÖ **Actualizar modelo semestralmente** con nuevos datos
6. ‚ö†Ô∏è **No usar para propiedades <$10k o fuera de Quito urbano**
7. ‚ö†Ô∏è **Validar espacialmente** antes de decisiones masivas

---

# Contribuciones Clave del Proyecto

1. **Sistema autom√°tico de detecci√≥n de leakage** (metodolog√≠a replicable)
2. **Transformaci√≥n logar√≠tmica** para normalizaci√≥n extrema (101.17 ‚Üí 0.29)
3. **Feature engineering geoespacial** espec√≠fico para Ecuador
4. **Pipeline end-to-end reproducible** para catastros
5. **Validaci√≥n espacial del error** con exportaci√≥n GIS
6. **Benchmark para Ecuador:** R¬≤ = 0.9605 con 60 features

---

# Publicaciones Futuras

## Art√≠culos Planificados

1. **"Automatic Target Leakage Detection in Real Estate Valuation"**
   - Venue: Journal of Real Estate Research

2. **"Log-Transform and Feature Selection for Cadastral Valuation"**
   - Venue: Computers, Environment and Urban Systems

3. **"Spatial Error Analysis in ML-based Property Valuation"**
   - Venue: International Journal of Geographical Information Science

---

<!-- _class: lead -->
<!-- _paginate: false -->

# ¬øPreguntas?

**Fausto Guano**
Universidad Yachay Tech
Maestr√≠a en Ciencia de Datos

üìß fausto.guano@yachaytech.edu.ec

---

# Referencias (1/2)

1. **Breiman, L. (2001).** Random forests. *Machine Learning*, 45(1), 5-32.

2. **Chen, T., & Guestrin, C. (2016).** XGBoost: A scalable tree boosting system. *Proceedings of KDD*, 785-794.

3. **Ke, G., et al. (2017).** LightGBM: A highly efficient gradient boosting decision tree. *Advances in NIPS*, 3146-3154.

4. **Pedregosa, F., et al. (2011).** Scikit-learn: Machine learning in Python. *JMLR*, 12, 2825-2830.

---

# Referencias (2/2)

5. **Arribas-Bel, D., Garcia-L√≥pez, M. √Ä., & Viladecans-Marsal, E. (2019).** Building(s and) cities: Delineating urban areas with a machine learning algorithm. *Journal of Urban Economics*, 103217.

6. **Hong, J., Choi, H., & Kim, W. S. (2020).** A house price valuation based on the random forest approach. *International Journal of Strategic Property Management*, 24(3), 140-152.

7. **Kaufman, S., Rosset, S., & Perlich, C. (2012).** Leakage in data mining: Formulation, detection, and avoidance. *ACM TKDD*, 6(4), 1-21.

---

# Ap√©ndice A: C√≥digo - Detecci√≥n de Leakage

```python
def detect_leakage(self, df, target_col):
    """Detecta target leakage por nombre y correlaci√≥n"""
    suspicious_cols = []

    # 1. Detecci√≥n por nombre
    name_patterns = ['valor', 'valoracion', 'avaluo',
                     'precio', 'costo', 'monto']
    for col in df.columns:
        if any(p in col.lower() for p in name_patterns):
            if col != target_col:
                suspicious_cols.append(col)

    # 2. Detecci√≥n por correlaci√≥n
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    correlations = df[numeric_cols].corr()[target_col].abs()
    high_corr = correlations[correlations > 0.8].index.tolist()

    suspicious_cols.extend([c for c in high_corr
                           if c not in suspicious_cols
                           and c != target_col])

    return list(set(suspicious_cols))
```

---

# Ap√©ndice B: C√≥digo - Transformaci√≥n Logar√≠tmica

```python
def limpiar_y_transformar_target(y, umbral_min=10000):
    """Limpia outliers y aplica log-transform"""
    
    # 1. Filtrar valores m√≠nimos problem√°ticos
    mask_validos = y >= umbral_min
    y_limpio = y[mask_validos]
    
    # 2. Filtrar outliers extremos (P1, P99)
    p1, p99 = np.percentile(y_limpio, [1, 99])
    mask_rango = (y_limpio >= p1) & (y_limpio <= p99)
    y_final = y_limpio[mask_rango]
    
    # 3. Aplicar transformaci√≥n logar√≠tmica
    y_log = np.log(y_final)
    
    # 4. Verificar mejora en asimetr√≠a
    asimetria_antes = skew(y)
    asimetria_despues = skew(y_log)
    
    return y_log, mask_validos & mask_rango
```

---

# Ap√©ndice C: M√©tricas Detalladas por Percentil

## Distribuci√≥n Real del Error (Test Set)

| Percentil | Error Absoluto | % del Valor Mediano |
|-----------|---------------|---------------------|
| 10% | $5,800 | 6.4% |
| 25% | $9,200 | 10.1% |
| **50%** | **$13,500** | **14.8%** |
| 75% | $28,900 | 31.7% |
| 90% | $62,400 | 68.5% |
| 95% | $108,000 | 118.5% |
| 99% | $345,000 | 378.6% |

---

# Ap√©ndice D: Detalles de Preprocesamiento

## Transformaciones Aplicadas

| Paso | Acci√≥n | Par√°metros |
|------|--------|------------|
| **Imputaci√≥n** | KNNImputer | k=5, weights='uniform' |
| **Encoding** | OneHotEncoder | drop='first', sparse=False |
| **Scaling** | StandardScaler | mean=0, std=1 |
| **Outliers** | IQR Method + P1-P99 | threshold=1.5 * IQR |
| **Target Transform** | Log-transform | np.log(y) |
| **CV** | StratifiedKFold | n_splits=5, shuffle=True |

---

# Ap√©ndice E: Hiperpar√°metros del Mejor Modelo

## RandomForest (Modelo Final)

```python
RandomForestRegressor(
    n_estimators=100,           # N√∫mero de √°rboles
    max_depth=None,             # Profundidad sin l√≠mite
    min_samples_split=2,        # M√≠nimo para split
    min_samples_leaf=1,         # M√≠nimo en hoja
    max_features='sqrt',        # Features por split
    bootstrap=True,             # Muestreo con reemplazo
    random_state=42,            # Reproducibilidad
    n_jobs=-1                   # Paralelizaci√≥n
)
```

**Tiempo de entrenamiento:** ~2 minutos  
**Predicci√≥n (8,882 registros):** <1 segundo

---

<!-- _class: lead -->
<!-- _paginate: false -->

# Gracias por su atenci√≥n

## Contacto
**Fausto Guano**
Universidad Yachay Tech

**Repositorio:** [GitHub - prediccion_avaluos_v2](https://github.com/usuario/proyecto_avaluos)

---